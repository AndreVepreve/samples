{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9abee3b",
   "metadata": {},
   "source": [
    "# Probability & Statistics for ML — Companion Notebook\n",
    "\n",
    "Use this notebook alongside `probability_statistics_for_ml.md`. Each section includes runnable code, short exercises, and reference links to official docs.\n",
    "\n",
    "**Note:** Some packages may be needed: `numpy`, `scipy`, `statsmodels`, `scikit-learn`, `matplotlib`. Install them in your environment as required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dedd9",
   "metadata": {},
   "source": [
    "## 1) Probability foundations: LTP & Bayes\n",
    "- Law of Total Probability and Bayes’ rule appear everywhere in ML—Naive Bayes, filtering, inference.\n",
    "\n",
    "Refs: SciPy/Stats docs and standard probability texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Toy disease screening example for Bayes rule\n",
    "p_disease = 0.01\n",
    "sens = 0.95   # P(+ | disease)\n",
    "spec = 0.98   # P(- | no disease)\n",
    "\n",
    "N = 1_000_000\n",
    "d = rng.random(N) < p_disease\n",
    "test_plus = np.empty(N, dtype=bool)\n",
    "test_plus[d] = rng.random(d.sum()) < sens\n",
    "test_plus[~d] = rng.random((~d).sum()) < (1-spec)\n",
    "\n",
    "# Posterior P(disease | +) via simulation\n",
    "post_sim = (d & test_plus).sum() / test_plus.sum()\n",
    "\n",
    "# Analytical via Bayes:\n",
    "# P(d|+) = P(+|d)P(d) / [P(+|d)P(d) + P(+|~d)P(~d)]\n",
    "post_bayes = (sens*p_disease) / (sens*p_disease + (1-spec)*(1-p_disease))\n",
    "post_sim, post_bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edda5c",
   "metadata": {},
   "source": [
    "**Exercise 1 (Bayes):** Change prevalence, sensitivity, and specificity; observe how the posterior \\(P(d\\mid +)\\) reacts.\n",
    "Add a small function that returns the posterior for arbitrary inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6ddba",
   "metadata": {},
   "source": [
    "## 2) Core distributions & the law of rare events\n",
    "**Poisson as a Binomial limit:** With \\(n\\to\\infty, p\\to 0, np=\\lambda\\), Binomial converges to Poisson(\\(\\lambda\\)).\n",
    "Refs: Poisson distribution background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "\n",
    "def pmf_poisson(lmbda, k):\n",
    "    return np.exp(-lmbda) * (lmbda**k) / factorial(k)\n",
    "\n",
    "def pmf_binom_approx(lmbda, k, n=10_000):\n",
    "    p = lmbda/n\n",
    "    # Use log-sum trick for stability for large n via logs if desired; here k is small for demo\n",
    "    from math import comb\n",
    "    return comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
    "\n",
    "lam = 3.2\n",
    "k_vals = np.arange(0, 12)\n",
    "po = np.array([pmf_poisson(lam, k) for k in k_vals])\n",
    "bi = np.array([pmf_binom_approx(lam, k, n=50_000) for k in k_vals])\n",
    "np.column_stack([k_vals, po, bi, np.abs(po-bi)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786325a",
   "metadata": {},
   "source": [
    "**Exercise 2 (memorylessness):** Simulate exponential waiting times and empirically verify \\(P(X>s+t\\mid X>s)=P(X>t)\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0312a35",
   "metadata": {},
   "source": [
    "## 3) Estimation: CIs for means & proportions\n",
    "### Welch's t-interval for mean difference\n",
    "Ref: `scipy.stats.ttest_ind` (Welch via `equal_var=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "x = rng.normal(0.0, 1.2, 80)\n",
    "y = rng.normal(0.4, 1.7, 120)\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(x, y, equal_var=False)\n",
    "\n",
    "nx, ny = len(x), len(y)\n",
    "vx, vy = x.var(ddof=1), y.var(ddof=1)\n",
    "se = np.sqrt(vx/nx + vy/ny)\n",
    "df = (vx/nx + vy/ny)**2 / ((vx**2/((nx**2)*(nx-1))) + (vy**2/((ny**2)*(ny-1))))\n",
    "delta = stats.t.ppf(0.975, df) * se\n",
    "ci = (x.mean()-y.mean()-delta, x.mean()-y.mean()+delta)\n",
    "t_stat, p_val, ci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f76737",
   "metadata": {},
   "source": [
    "### Proportion CIs (Wilson, Clopper–Pearson)\n",
    "Ref: `statsmodels.stats.proportion.proportion_confint` (methods: `'wilson'`, `'beta'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351dbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "count, n = 37, 200\n",
    "ci_wilson = proportion_confint(count, n, method=\"wilson\")\n",
    "ci_exact  = proportion_confint(count, n, method=\"beta\")  # Clopper–Pearson\n",
    "ci_wilson, ci_exact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401ab3d",
   "metadata": {},
   "source": [
    "**Exercise 3 (coverage):** Monte‑carlo compare Wilson vs. Wald coverage for small \\(p\\) (e.g., \\(p=0.05\\), \\(n=50\\)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612680c",
   "metadata": {},
   "source": [
    "## 4) Resampling: bootstrap & permutation tests\n",
    "Refs: SciPy `bootstrap` and `permutation_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "rng = np.random.default_rng(0)\n",
    "sample = rng.normal(loc=0.0, scale=1.0, size=500)\n",
    "\n",
    "# Bootstrap CI for the mean (BCa)\n",
    "res = stats.bootstrap((sample,), np.mean, n_resamples=10_000, method=\"BCa\")\n",
    "res.confidence_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Permutation test on difference in means\n",
    "rng = np.random.default_rng(1)\n",
    "a = rng.lognormal(mean=0.0, sigma=1.0, size=60)\n",
    "b = rng.lognormal(mean=0.2, sigma=1.2, size=80)\n",
    "\n",
    "def stat_fn(x, y, axis):\n",
    "    return x.mean(axis=axis) - y.mean(axis=axis)\n",
    "\n",
    "perm = stats.permutation_test((a, b), stat_fn, n_resamples=100_000,\n",
    "                              alternative=\"two-sided\", random_state=rng)\n",
    "perm.pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407b64d",
   "metadata": {},
   "source": [
    "**Exercise 4 (robust stat):** Redo the permutation test using the median as the statistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262c291",
   "metadata": {},
   "source": [
    "## 5) A/B design: power & multiple testing\n",
    "Refs: `statsmodels.stats.power.TTestIndPower` / `NormalIndPower`; `statsmodels.stats.multitest.fdrcorrection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c937997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import numpy as np\n",
    "\n",
    "# Power / sample size\n",
    "t_power = TTestIndPower().power(effect_size=0.3, nobs1=200, alpha=0.05)\n",
    "n_per_arm = TTestIndPower().solve_power(effect_size=0.3, power=0.8, alpha=0.05)\n",
    "\n",
    "# FDR across many p-values (Benjamini–Hochberg)\n",
    "pvals = np.array([0.03, 0.20, 0.002, 0.08, 0.049])\n",
    "reject, p_adj = fdrcorrection(pvals, alpha=0.05, method=\"indep\")\n",
    "t_power, n_per_arm, reject, p_adj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b4a3d",
   "metadata": {},
   "source": [
    "**Exercise 5 (proportions):** Convert a desired absolute lift in conversion (p0→p1) into Cohen’s *h* and compute required \\(n\\) via `NormalIndPower`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb81ce",
   "metadata": {},
   "source": [
    "## 6) Classifier metrics: ROC/PR, calibration, Brier\n",
    "Refs: scikit‑learn `roc_auc_score`, `precision_recall_curve`, `average_precision_score`, `calibration_curve`, `brier_score_loss`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9475fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "X, y = make_classification(n_samples=4000, n_features=20, weights=[0.85,0.15], random_state=0)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(Xtr, ytr)\n",
    "p_raw = clf.predict_proba(Xte)[:,1]\n",
    "auc = roc_auc_score(yte, p_raw)\n",
    "precision, recall, thr = precision_recall_curve(yte, p_raw)\n",
    "ap = average_precision_score(yte, p_raw)\n",
    "brier_raw = brier_score_loss(yte, p_raw)\n",
    "\n",
    "cal = CalibratedClassifierCV(base_estimator=clf, method='isotonic', cv=3).fit(Xtr, ytr)\n",
    "p_cal = cal.predict_proba(Xte)[:,1]\n",
    "brier_cal = brier_score_loss(yte, p_cal)\n",
    "\n",
    "auc, ap, brier_raw, brier_cal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa27c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reliability diagram (plot)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for p, lbl in [(p_raw, \"raw\"), (p_cal, \"isotonic\")]:\n",
    "    prob_true, prob_pred = calibration_curve(yte, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f\"{lbl} (Brier={brier_score_loss(yte,p):.3f})\")\n",
    "plt.plot([0,1],[0,1],'--',alpha=.5,label='perfect')\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Empirical frequency\")\n",
    "plt.legend(); plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82c4b7",
   "metadata": {},
   "source": [
    "**Exercise 6 (imbalance):** Change class weights and observe the effect on PR curve and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b9b77",
   "metadata": {},
   "source": [
    "## 7) Monitoring: covariate shift (KS) and PSI\n",
    "Refs: SciPy `ks_2samp`; industry references for PSI definition and thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def psi(expected, actual, bins=10):\n",
    "    qs = np.quantile(expected, np.linspace(0,1,bins+1))\n",
    "    ex, _ = np.histogram(expected, bins=qs)\n",
    "    ac, _ = np.histogram(actual,   bins=qs)\n",
    "    ex = np.where(ex==0, 1, ex); ac = np.where(ac==0, 1, ac)\n",
    "    ex_pct, ac_pct = ex/ex.sum(), ac/ac.sum()\n",
    "    return np.sum((ac_pct - ex_pct) * np.log(ac_pct / ex_pct))\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "train = rng.normal(0,1, 4000)\n",
    "live  = rng.normal(0.2,1.1, 4000)\n",
    "\n",
    "ks_stat, ks_p = ks_2samp(train, live)\n",
    "psi_val = psi(train, live)\n",
    "ks_stat, ks_p, psi_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a7e72",
   "metadata": {},
   "source": [
    "**Exercise 7 (alerting):** Choose sensible PSI and KS thresholds for your domain and simulate weekly drift scenarios. Log decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981ff0e",
   "metadata": {},
   "source": [
    "## References\n",
    "- SciPy bootstrap: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html\n",
    "- SciPy permutation_test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.permutation_test.html\n",
    "- StatsModels proportion_confint: https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportion_confint.html\n",
    "- StatsModels power: https://www.statsmodels.org/stable/generated/statsmodels.stats.power.TTestIndPower.html\n",
    "- scikit‑learn calibration_curve: https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html\n",
    "- scikit‑learn ROC AUC: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "- scikit‑learn precision_recall_curve: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "- SciPy ks_2samp: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n",
    "- PSI background (SAS/IBM): https://support.sas.com/resources/papers/proceedings10/288-2010.pdf , https://www.ibm.com/think/topics/model-drift\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}